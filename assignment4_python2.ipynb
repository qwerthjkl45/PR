{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic Robotics: Assignment 5\n",
    "\n",
    "A Particle Filter is a popular method used for simultaneous localization and mapping, because it is\n",
    "relatively easy to implement, has computational advantages and is robust against data association\n",
    "problems. Applying this to a scenario where only a limited number of landmarks are visible (i.e.\n",
    "six colored landmarks around a soccer field), should be simple.\n",
    "\n",
    "Yet, this time the algorithm has to be implemented to real robots. The observations of range\n",
    "and bearing are not provided, but should be estimated from the camera images. This means that\n",
    "the landmarks have to be detected, classified and localized in the image under changing lightingconditions.\n",
    "This location in the image has to be converted to a vector in the local coordination\n",
    "system by including the position of the head and the body of the robot. So, there will be a lot of\n",
    "hard work, frustration and fun to get the particle filter working on the real robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field of the RoboCup Soccer as used in the experiment. The Nao robot will walked\n",
    "several times the 8-shaped path marked on the field:\n",
    "<img src=https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/fieldSetup.jpg width=\"500\">\n",
    "\n",
    "For this experiment a configuration inspired the experiment by Steffen Gutmann is used [1]. Steffen\n",
    "directed an Aibo over the field with a joy-stick, and recorded the odometry updates and the\n",
    "observations of the landmarks of the Aibo. It is your task to do the same with a Nao robot. The\n",
    "Nao has a limited field of view, so only 1-2 landmarks are visible at the same time. The clue to\n",
    "detect landmarks is the pink band each landmark has. By looking above and below this band you\n",
    "should be able to reliable classify the signature of the landmark.\n",
    "Around the field six landmarks are visible, color coded with bands of pink combined with green,\n",
    "yellow and blue (see Fig. ). The green landmarks are placed on locations (0,215) and (0,-215) cm.\n",
    "The yellow landmarks are located at (315,215) and (315,-215) cm. The blue landmarks are placed\n",
    "at (-315,215) and (-315,-215) cm. Note that you make the detection of the landmarks easier when you put white boards behind the landmarks.\n",
    "\n",
    "These are the landmarks which are placed around the RoboCup Soccer field:\n",
    "<img src=https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/Landmarks.jpg width=\"500\">\n",
    "\n",
    "On the field five black marks are made with black tape. Because the size of the soccer field is 6x4\n",
    "meter, twice as large as the field used in the Gutmann dataset [1], the marks are placed further apart\n",
    "(see Fig. ). The marks are placed in an eight-shape figure on locations (0,0), (0,100), (100,-100),\n",
    "(-150,0) and (-150,100)cm.\n",
    "\n",
    "The layout of the RoboCup Soccer field, with the markers on the field and the landmarks\n",
    "around the field:\n",
    "<img src=https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/FieldLayout.png width=\"500\">\n",
    "\n",
    "For your task a number of Nao humanoid robots with NaoQi are available: Tom, Jerry, Brooke, Mio, Eve, Walle, Sam and Moos. The other Nao robots in the lab have an hardware problem or are running the Dutch Nao Team\n",
    "code. Note that Sam and Moos are v6 robots, while the other robots are v5 robots. The version is important for your choice of the software (NaoQi). An extensive introduction how to use the robot are available:\n",
    "\n",
    "[extensive introduction](https://github.com/Caiit/DNT-Introduction/raw/master/DNT_Introduction.pdf)\n",
    "\n",
    "Most important to remember the recommendation from Section 3.2 of the extensive introduction how to work with the robots:\n",
    "\n",
    ">\"Working with robots takes special care. Always work with at least two people\n",
    "when working with the robot, since it is not that stable and can fall over a\n",
    "lot. Make sure that one person is always standing next to the robot to catch it\n",
    "when it falls and the other one behind the laptop to start and stop the code.\n",
    "Furthermore, when not using the robot, let it go to its rest position by pressing\n",
    "twice on its chest button. To undo this resting mode, again press twice on its\n",
    "chest button. Also, always put the charger in the back of the robot when not\n",
    "using it, its battery is not that great.\"\n",
    "\n",
    "Remember that you can work in groups of 4 on this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Data Acquisition\n",
    "\n",
    "Create a dataset by the following code or by combining the following Python scripts:\n",
    "* [a script](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2017/NaoSLAM/keyboard_control.py) which drive the robot around with the keyboard (Courtesy  Michiel van der Meer and Caitlin Lagrand) \n",
    "* [a script](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2017/NaoSLAM/datasetCreator.py) to take pictures from the top camera (Courtesy Douwe van der Wal)\n",
    "* you also need [the naomanager.py script](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2017/NaoSLAM//naomanager.py) (Courtesy Martin Stolk)\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections \n",
    "import time\n",
    "import os\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "from numpy.random import choice\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import urllib2\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should print the string '2.7.15rc1'. If it prints the string '3.6.6' you should do the following in the bash shell:\n",
    "\n",
    "<tt>sudo python2 -m pip install ipykernel</tt>\n",
    "\n",
    "followed by\n",
    "\n",
    "<tt>python2 -m ipykernel install --user</tt>\n",
    "\n",
    "Restart jupyter notebook and check if the python version is '2.7', which is required for NAOqi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Getch:\n",
    "    \"\"\"Gets a single character from standard input.  Does not echo to the\n",
    "screen.\"\"\"\n",
    "    def __init__(self):\n",
    "        try:\n",
    "            self.impl = _GetchWindows()\n",
    "        except ImportError:\n",
    "            self.impl = _GetchUnix()\n",
    "\n",
    "    def __call__(self): return self.impl()\n",
    "\n",
    "\n",
    "class _GetchUnix:\n",
    "    def __init__(self):\n",
    "        import tty, sys\n",
    "\n",
    "    def __call__(self):\n",
    "        import sys, tty, termios\n",
    "        fd = sys.stdin.fileno()\n",
    "        old_settings = termios.tcgetattr(fd)\n",
    "        try:\n",
    "            tty.setraw(sys.stdin.fileno())\n",
    "            ch = sys.stdin.read(1)\n",
    "        finally:\n",
    "            termios.tcsetattr(fd, termios.TCSADRAIN, old_settings)\n",
    "        return ch\n",
    "\n",
    "\n",
    "class _GetchWindows:\n",
    "    def __init__(self):\n",
    "        import msvcrt\n",
    "\n",
    "    def __call__(self):\n",
    "        import msvcrt\n",
    "        return msvcrt.getch()\n",
    "\n",
    "\n",
    "getch = _Getch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "++++++++++++++++++++++++++++Now you can download the SDKs to interface with the robot. You need both the Python and C++ SDK.\n",
    "Go to\n",
    "\n",
    "> [Softbank Robotics Community Resources](https://community.ald.softbankrobotics.com/en/resources/software/language/en-gb/robot/nao-2/field_software_type/sdk)\n",
    "\n",
    "<img src=https://staff.fnwi.uva.nl/a.visser/research/nao/2018/NAOqiSDK.png width=\"500\">\n",
    "\n",
    "You could ask for the credentials from your TA. Alternatively, you could download the SDKs from:\n",
    "\n",
    "> [Dutch Nao Team shared SDKs] \n",
    "(https://drive.google.com/open?id=1rGhWUx7I0UUsOKF0R72lVjt5LYPv-4Y3)\n",
    "\n",
    "Download the Python 2.7 SDK 2.1.4 Linux 64 and the C++ SDK 2.1.4 Linux 64.\n",
    "\n",
    "Install both SDKs in a directory <tt>packages</tt>. Remember the location of the directory, you need that location for jupyter and your bash shell.\n",
    "\n",
    "Edit your <tt>~/.bash_profile</tt>. Add a few lines. First line to add is:\n",
    "```\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/</tt>\n",
    "```\n",
    "\n",
    "\n",
    "Followed by\n",
    "```\n",
    "# NaoQi 2.1.4.13 for Nao v5\n",
    "export PYTHONPATH=$PYTHONPATH:$HOME/packages/aldabaran/pynaoqi-python2.7-2.1.4.13-linux64\n",
    "export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$HOME/packages/aldebaran/naoqi-sdk-2.1.4.13-linux64/lib:$HOME/packages/aldabaran/pynaoqi-python2.7-2.1.4.13-linux64\n",
    "export JUPYTER_PATH=/$HOME/packages/aldabaran/pynaoqi-python2.7-2.1.4.13-linux64\n",
    "```\n",
    "\n",
    "Load this profile with the command <tt>source ~/.bashrc</tt>. Check it with <tt>env | grep PATH</tt>\n",
    "\n",
    "Restart the Jupyter notebook and check if the following code loads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/qwerthjkl45/packages/pynaoqi-python2.7-2.4.3.28-linux64\")\n",
    "from naoqi import ALProxy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PORT = 9559\n",
    "\n",
    "class Nao(object):\n",
    "    \"\"\" Represents a Nao. Initializes the proxies we want.\n",
    "        Additionally provides methods for logging the angles.\"\"\"\n",
    "    def __init__(self, ip, port=9559, logfolder=None):\n",
    "        super(Nao, self).__init__()\n",
    "\n",
    "        self.ip = ip\n",
    "        self.port = port\n",
    "\n",
    "        self.motion = ALProxy('ALMotion', ip, port)\n",
    "        self.behavior = ALProxy('ALBehaviorManager', ip, port)\n",
    "        self.posture = ALProxy('ALRobotPosture', ip ,port)\n",
    "\n",
    "        self.jointnames = self.motion.getJointNames(\"Body\")\n",
    "        self.timestamps = []\n",
    "        self.desiredangles = defaultdict( lambda : list() )\n",
    "        self.realangles = defaultdict( lambda : list() )\n",
    "\n",
    "        self.initstiffness()\n",
    "\n",
    "        # Create logging folder if needed\n",
    "        self.logfolder = logfolder\n",
    "        self.logfp = None\n",
    "        if logfolder:\n",
    "            if not os.path.exists(logfolder):\n",
    "                os.mkdir(logfolder)\n",
    "\n",
    "    def initstiffness(self):\n",
    "        self.motion.stiffnessInterpolation('Body', 0.5, 1.0)\n",
    "\n",
    "    def releasestiffness(self):\n",
    "        self.motion.stiffnessInterpolation('Body', 0.0, 1.0)\n",
    "\n",
    "    def startlogging(self):\n",
    "        if not self.logfolder:\n",
    "            return\n",
    "        now = datetime.datetime.now()\n",
    "        self.logname = '%s_%s-%.4d%.2d%.2d%.2d%.2d%.2d' % (self.ip, self.port, now.year, now.month, now.day, now.hour, now.minute, now.second)\n",
    "        self.logfp = open(os.path.join(self.logfolder, self.logname), 'wb')\n",
    "\n",
    "        # First line of the log file contains the joint names\n",
    "        self.logfp.write('timestamp')\n",
    "        for n in self.jointnames:\n",
    "            self.logfp.write('\\t%s\\t ' % (n))\n",
    "        self.logfp.write('\\n')\n",
    "\n",
    "    def stoplogging(self):\n",
    "        if self.logfp:\n",
    "            self.logfp.close()\n",
    "            self.logfp = None\n",
    "\n",
    "    def stop(self):\n",
    "        self.releasestiffness()\n",
    "        self.motion = None\n",
    "        self.behavior = None\n",
    "        self.posture = None\n",
    "        self.stoplogging()\n",
    "\n",
    "    def updateanglehistory(self):\n",
    "        self.timestamps.append(time.time())\n",
    "        if self.logfolder:\n",
    "            self.logfp.write('%f' % (self.timestamps[-1]))\n",
    "\n",
    "        n = len(self.timestamps)\n",
    "        self.timestamps = self.timestamps[max(0,n-100):n]\n",
    "\n",
    "        angles = self.motion.getAngles('Body', False)\n",
    "        anglesreal = self.motion.getAngles('Body', True)\n",
    "        for i, name in enumerate(self.jointnames):\n",
    "            self.desiredangles[name].append(angles[i])\n",
    "            self.realangles[name].append(anglesreal[i])\n",
    "\n",
    "            n = len(self.desiredangles[name])\n",
    "            self.desiredangles[name] = self.desiredangles[name][max(0,n-100):n]\n",
    "            n = len(self.realangles[name])\n",
    "            self.realangles[name] = self.realangles[name][max(0,n-100):n]\n",
    "\n",
    "            if self.logfolder:\n",
    "                self.logfp.write('\\t%f\\t%f' % (angles[i], anglesreal[i]))\n",
    "        if self.logfolder: self.logfp.write('\\n')\n",
    "            \n",
    "\n",
    "    def gettimestamps(self): return self.timestamps\n",
    "\n",
    "class NaoCallAll(object):\n",
    "    \"\"\" Wraps a call to multiple Naos/attributes.\n",
    "        Returns a new call wrapper in case you try to access an attribute.\"\"\"\n",
    "    def __init__(self, objects, attr):\n",
    "        super(NaoCallAll, self).__init__()\n",
    "\n",
    "        self.attrobjects = []\n",
    "\n",
    "        for o in objects:\n",
    "            self.attrobjects.append( o.__getattribute__(attr) )\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "\n",
    "        return NaoCallAll(self.attrobjects, attr)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        results = []\n",
    "        for a in self.attrobjects:\n",
    "            results.append( a(*args, **kwargs) )\n",
    "        return results\n",
    "\n",
    "    def __getitem__(self, k):\n",
    "        results = []\n",
    "        for a in self.attrobjects:\n",
    "            results.append( a.__getitem__(k) )\n",
    "        return results\n",
    "\n",
    "\n",
    "class NaoManager(list):\n",
    "    \"\"\" Manages a list of nao's.\n",
    "        Works like a normal list, except a call on the\n",
    "        list will result in the method being called on all nao's in the list.\"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(NaoManager, self).__init__(*args, **kwargs)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        return NaoCallAll(self, attr)\n",
    "\n",
    "    def addnao(self, ip, port=9559, logfolder=None):\n",
    "        self.append( Nao(ip, port, logfolder) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "keyboard_control.py:\n",
    "    Control a Nao robot that is currently running the Naoqi software with your\n",
    "    keyboard! Key bindings are defined in controllers.txt.\n",
    "\n",
    "    Requires naomanager.py & an installation of pynaoqi\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "#import getch\n",
    "import json\n",
    "from threading import Thread\n",
    "\n",
    "#from naomanager import NaoManager, DEFAULT_PORT\n",
    "from naoqi import ALProxy\n",
    "\n",
    "__author__ = \"Michiel van der Meer, Caitlin Lagrand\"\n",
    "__copyright__ = \"Copyright 2017, Dutch Nao Team\"\n",
    "__version__ = \"0.2.0\"\n",
    "\n",
    "#getch = getch._Getch()\n",
    "\n",
    "naos = NaoManager()\n",
    "\n",
    "def load_motion(name):\n",
    "    print(\"Performing motion {}\".format(name))\n",
    "    with open(\"animations/{}.json\".format(name), \"r\") as f:\n",
    "        fdict = json.load(f)\n",
    "        fdict[\"names\"] = [str(x) for x in fdict[\"names\"]]\n",
    "    return fdict\n",
    "\n",
    "\n",
    "# Main program to perform actions\n",
    "def main(args):\n",
    "    # Create list of nao objects\n",
    "    \n",
    "    if hasattr(args, 'nao'):\n",
    "        for nao in args.nao:\n",
    "            try:\n",
    "                ip, port = nao.split(':')\n",
    "                # naos.addnao(ip, int(port))\n",
    "            except ValueError:\n",
    "                ip = nao\n",
    "                port = DEFAULT_PORT\n",
    "                naos.addnao(ip, int(port))\n",
    "    else:\n",
    "        naos.add(\"nao.local\", DEFAULT_PORT)\n",
    "    \n",
    "    if len(naos) == 1:\n",
    "        print(\"Connected {} Nao\".format(len(naos)))\n",
    "    else:\n",
    "        print(\"Connected {} Naos\".format(len(naos)))\n",
    "    x = 0.0\n",
    "    y = 0.0\n",
    "    theta = 0.0\n",
    "    frequency = 0.3\n",
    "    CommandFreq = 0.5\n",
    "    starttime = time.time()\n",
    "    print(\"Ready for liftoff at: {}\".format(starttime))\n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"moos.local\")\n",
    "\n",
    "for nao in naos:\n",
    "    nao.startlogging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP = \"moos.local\"\n",
    "PORT = 9559\n",
    "\n",
    "def start_proxies():\n",
    "    '''\n",
    "    Start the motion and posture proxy and let the robot stand up.\n",
    "    '''\n",
    "    motion_proxy  = ALProxy(\"ALMotion\", IP, PORT)\n",
    "    posture_proxy = ALProxy(\"ALRobotPosture\", IP, PORT)\n",
    "    memory_proxy  = ALProxy(\"ALMemory\", IP, PORT)\n",
    "\n",
    "    motion_proxy.wakeUp()\n",
    "    posture_proxy.goToPosture(\"StandInit\", 0.5)\n",
    "    motion_proxy.setAngles(\"HeadPitch\", 0.3, 0.1)\n",
    "\n",
    "    return motion_proxy, posture_proxy, memory_proxy\n",
    "\n",
    "def create_video_connection(ip=None, port=None, camera=0):\n",
    "    '''\n",
    "    Create a connection with the robot and start a camera proxy.\n",
    "    returns the video device and the capture device.\n",
    "    https://gist.github.com/takamin/990aa0133919aa58944d\n",
    "    '''\n",
    "    if ip == None: ip = IP\n",
    "    if port == None: port = PORT\n",
    "    # Create proxy to nao\n",
    "    cam_proxy = ALProxy(\"ALVideoDevice\", ip, port)\n",
    "    AL_kTopCamera = camera # 0: topcamera, 1: bottomcamera\n",
    "    AL_kQVGA = 1      # 320x240\n",
    "    AL_kBGRColorSpace = 13\n",
    "    return cam_proxy, cam_proxy.subscribeCamera(\"top\", AL_kTopCamera, AL_kQVGA, AL_kBGRColorSpace, 10)\n",
    "\n",
    "\n",
    "def get_img_from_robot(video_device, capture_device):\n",
    "    '''\n",
    "    Get a frame from the robot and return it as numpy array.\n",
    "    https://gist.github.com/takamin/990aa0133919aa58944d\n",
    "    '''\n",
    "    # Create image\n",
    "    width = 320\n",
    "    height = 240\n",
    "    img = np.zeros((height, width, 3), np.uint8)\n",
    "    captured = video_device.getImageRemote(capture_device)\n",
    "    if captured == None:\n",
    "        print(\"Cannot capture\")\n",
    "    elif captured[6] == None:\n",
    "        print(\"No image data string\")\n",
    "    else:\n",
    "        # translate value to mat\n",
    "        values = map(ord, list(captured[6]))\n",
    "        i = 0\n",
    "        for y in range(0, height):\n",
    "            for x in range(0, width):\n",
    "                img.itemset((y, x, 0), values[i + 0])\n",
    "                img.itemset((y, x, 1), values[i + 1])\n",
    "                img.itemset((y, x, 2), values[i + 2])\n",
    "                i += 3\n",
    "    return img\n",
    "\n",
    "\n",
    "motion_proxy, posture_proxy, memory_proxy = start_proxies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "width = 320\n",
    "height = 240\n",
    "image = np.zeros((height, width, 3), np.uint8)\n",
    "num = 200\n",
    "\n",
    "video_device, capture_device = create_video_connection(IP, PORT, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = memory_proxy.getDataList(\"Device/Laser/Value\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If ```import cv2``` fails, quit jupyter and execute the command:\n",
    "\n",
    "```\n",
    "sudo pip install opencv-contrib-python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# capture an image\n",
    "# import almath\n",
    "\n",
    "for nao in naos:\n",
    "    nao.updateanglehistory()\n",
    "   \n",
    "jointvalues = motion_proxy.getAngles(\"Body\", True)\n",
    "jointnames = motion_proxy.getBodyNames(\"Body\")\n",
    "position = motion_proxy.getRobotPosition(False)\n",
    "    \n",
    "result = video_device.getImageRemote(capture_device);\n",
    "if result == None:\n",
    "    print 'cannot capture.'\n",
    "elif result[6] == None:\n",
    "    print 'no image data string.'\n",
    "else: \n",
    "    print 'captured.'\n",
    "    values = map(ord, list(result[6]))\n",
    "    image = np.reshape(values, (height, width, 3)).astype('uint8')\n",
    "    cv2.imshow(\"top-camera-320x240\", image)\n",
    "    string2 = \"./plaatje\" + str(num) + \".jpg\"\n",
    "    cv2.imwrite(string2, image) \n",
    "    \n",
    "    string2 = \"./plaatje\" + str(num) + \".txt\"\n",
    "    file = open(string2,\"w\")\n",
    "    \n",
    "    file.write(\"Recording at estimated position: \" + str(position) + \"\\n\")\n",
    "    \n",
    "    for jointvalue, jointname in zip(jointvalues, jointnames):\n",
    "        file.write(jointname + \" \" + str(jointvalue) + \"\\n\")\n",
    "    file.close()\n",
    "    num += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_data(capture_device, motion_proxy, naos, num, mark = False):\n",
    "    file = open('datalog.txt', \"a\")\n",
    "    \n",
    "    if not mark:\n",
    "        position = motion_proxy.getRobotPosition(False)\n",
    "        lm, r, b = capture_an_image(capture_device, motion_proxy, naos, num)\n",
    "        #lm, r, b = detect_landmark('./plaatje100.jpg')\n",
    "        #position = [1, 2, 3] # list\n",
    "        file.write(\"obs: \" + str(position[0]) + \" \"  + str(position[1]) + \" \" + str(position[2]))\n",
    "        if lm == \"\":\n",
    "            file.write(\" \"+ str(0)+ \"\\n\")\n",
    "        else:\n",
    "            file.write(\" \"+ str(1) + \" ( \" + lm + \" \" + str(r) + \" \" + str(b) + \" )\"+ \"\\n\")\n",
    "    else:\n",
    "        file.write('mark\\n')\n",
    "    file.close()\n",
    "#record_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_an_image(capture_device, motion_proxy, naos, num):\n",
    "    lm = \"\"\n",
    "    for nao in naos:\n",
    "        nao.updateanglehistory()\n",
    "   \n",
    "    jointvalues = motion_proxy.getAngles(\"Body\", True)\n",
    "    jointnames = motion_proxy.getBodyNames(\"Body\")\n",
    "    position = motion_proxy.getRobotPosition(False)\n",
    "\n",
    "    result = video_device.getImageRemote(capture_device);\n",
    "    if result == None:\n",
    "        print 'cannot capture.'\n",
    "    elif result[6] == None:\n",
    "        print 'no image data string.'\n",
    "    else: \n",
    "        print 'captured.'\n",
    "        values = map(ord, list(result[6]))\n",
    "        image = np.reshape(values, (height, width, 3)).astype('uint8')\n",
    "        cv2.imshow(\"top-camera-320x240\", image)\n",
    "        string2 = \"./plaatje\" + str(num) + \".jpg\"\n",
    "        cv2.imwrite(string2, image)\n",
    "        lm, r, b = detect_landmark(string2)\n",
    "        \n",
    "    return lm,r, b\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to change capture_device blablabla\n",
    "#capture_device, motion_proxy, naos, \n",
    "num = 0\n",
    "while True:\n",
    "    key_press = raw_input()\n",
    "    if (key_press == 'w'):\n",
    "        print(\"Moving forward\")\n",
    "        x = 0.5; CommandFreq = 0.5; frequency = 0.3;\n",
    "        motion_proxy.setWalkTargetVelocity(x, 0, 0, frequency)\n",
    "        time.sleep(CommandFreq)\n",
    "        motion_proxy.setWalkTargetVelocity(0, 0, 0, frequency)\n",
    "        time.sleep(CommandFreq)\n",
    "    elif (key_press == 's'):\n",
    "        print(\"Standup\")\n",
    "        posture_proxy.goToPosture(\"StandInit\", 0.5)\n",
    "    elif (key_press == 'x'):\n",
    "        print(\"Stopping\")\n",
    "        x = 0.5; CommandFreq = 0.5; frequency = 0.3;\n",
    "        motion_proxy.setWalkTargetVelocity(0, 0, 0, frequency)\n",
    "        time.sleep(CommandFreq)\n",
    "    elif (key_press == 'a'):\n",
    "        print(\"Turning left\")\n",
    "        x = 0.0; y = 0.0; theta = 0.5; CommandFreq = 0.5; frequency = 0.3;\n",
    "        motion_proxy.setWalkTargetVelocity(x, y, theta, frequency)\n",
    "        time.sleep(CommandFreq)\n",
    "        motion_proxy.setWalkTargetVelocity(0, 0, 0, frequency)\n",
    "        time.sleep(CommandFreq)\n",
    "    elif (key_press == 'd'):\n",
    "        print(\"Turning right\")\n",
    "        x = 0.0; y = 0.0; theta = -0.5; CommandFreq = 0.5; frequency = 0.3;\n",
    "        motion_proxy.setWalkTargetVelocity(x, y, theta, frequency)\n",
    "        time.sleep(CommandFreq)\n",
    "        motion_proxy.setWalkTargetVelocity(0, 0, 0, frequency)\n",
    "        time.sleep(CommandFreq)\n",
    "    elif (key_press == 'r'):\n",
    "        print('record data')\n",
    "        record_data(capture_device, motion_proxy, naos, num)\n",
    "        num = num + 1\n",
    "    elif (key_press == 'm'):\n",
    "        print('record data mark')\n",
    "        record_data(capture_device, motion_proxy, naos, num, True)\n",
    "        num = num + 1\n",
    "    elif (key_press == 'z'):\n",
    "        print(\"Going to rest\")\n",
    "        motion_proxy.post.rest()\n",
    "        time.sleep(0.5)\n",
    "        print(\"Resting (zzz)\")\n",
    "        motion_proxy.stiffnessInterpolation('Body', 0.0, 1.0)\n",
    "    elif (key_press == 'c'):\n",
    "        print(\"Closing Connection\")\n",
    "        CommandFreq = 0.5;\n",
    "        \n",
    "        record_data(capture_device, motion_proxy, naos, num)\n",
    "        for nao in naos:\n",
    "            nao.stoplogging()\n",
    "            nao.motion.rest()\n",
    "            time.sleep(CommandFreq)\n",
    "            nao.motion.killAll()\n",
    "            nao.stop()\n",
    "        break\n",
    "        \n",
    "    elif (key_press == 'p'):\n",
    "        break\n",
    "    else:\n",
    "        print('doing nothing')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Detect the landmarks\n",
    "\n",
    "Find the landmarks by searching for colored regions. Check the size, shape and location above the horizon\n",
    "to be sure that you have a landmark. Check the color above and below the pink band to get the signature of the observation $s^i_t$. Estimate the range $r^i_t$ from the size of the pink object. Estimate the bearing $\\phi^i_t$ by combining the center of the object in the image with the HeadPitch angle of the Nao\n",
    "robot. Try to get statistics on the standard deviation of your observations.\n",
    "* you could use [the ColorRegion.py script](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2017/NaoSLAM//ColorRegion.py) (Courtesy Sebastian Thrun) as starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 -> green \n",
    "#1 -> magenta \n",
    "#2 -> yellow \n",
    "#3 -> blue\n",
    "def find_color(color_range, crop_img):\n",
    "    landmark = \"\"\n",
    "    x, y, _ = crop_img.shape\n",
    "    area_list = np.zeros((3, ))\n",
    "    y_list = np.zeros((3, ))\n",
    "    for idx in range(3):\n",
    "        lower = np.uint8(color_range[(idx*2) + 1, :])\n",
    "        upper = np.uint8(color_range[idx*2 , :])\n",
    "        mask2 = cv2.inRange(crop_img, lower, upper)\n",
    "        #mask2 = cv2.inRange(crop_img, gl, gh)\n",
    "        #mask2 = cv2.erode(mask2, kernel)\n",
    "        \n",
    "        kernel1 = np.ones((8, 8), \"uint8\")\n",
    "        mask2 = cv2.dilate(mask2, kernel1)\n",
    "        _, contours, _ = cv2.findContours(mask2,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "                \n",
    "        if len(contours) == 0:\n",
    "            continue\n",
    "        \n",
    "        cnt2 = max(contours, key = cv2.contourArea)\n",
    "        if (cv2.contourArea(cnt2) > 100):\n",
    "            x1, y1, _, _ = cv2.boundingRect(cnt2)\n",
    "            \n",
    "            area_list[idx] = cv2.contourArea(cnt2)\n",
    "            y_list[idx] = y1\n",
    "            \n",
    "   \n",
    "    if not (area_list == 0).all():\n",
    "        idx = np.argmax(area_list)\n",
    "        y1 = y_list[idx]\n",
    "        if idx == 0:\n",
    "            if y1 < y/2:\n",
    "                print('yp')\n",
    "                landmark = \"2:1\"\n",
    "            else:\n",
    "                print('py')\n",
    "                landmark = \"1:2\"\n",
    "        elif idx == 1:\n",
    "            if y1 < y/2:\n",
    "                print('bp')\n",
    "                landmark = \"0:1\"\n",
    "            else:\n",
    "                print('pb')\n",
    "                landmark = \"1:0\"\n",
    "        else:\n",
    "            if y1 < y/2:\n",
    "                print('gp')\n",
    "                landmark = \"3:1\"\n",
    "            else:\n",
    "                print('pg')\n",
    "                landmark = \"1:3\"\n",
    "    else:\n",
    "        print('detect nothing')\n",
    "\n",
    "    \n",
    "    return landmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_range_bearing(x,w, pic_width):\n",
    "    z_range = 30.0 * 1290.5/w #(mm)\n",
    "    \n",
    "    # calculate focal  length (pixel)\n",
    "    foc = np.tan((90 - (60.97/2)) * np.pi/(180)) * pic_width /2\n",
    "    try:\n",
    "        z_bearing = 90 - (np.arctan(float(foc) / abs(x + (w/2) - (pic_width/2)))* 180/np.pi)\n",
    "    except ZeroDivisionError:\n",
    "        z_bearing = 90 - (np.arctan(np.inf)* 180/np.pi)\n",
    "    return z_range, z_bearing\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_landmark(file_str):\n",
    "    lm = ''\n",
    "    r = -10000\n",
    "    b = -10000\n",
    "    \n",
    "    wh = np.array([50, 50, 50], np.uint8)\n",
    "    wl = np.array([0, 0, 0], np.uint8)\n",
    "\n",
    "\n",
    "    mgh = np.array([170 ,255 ,255], np.uint8)\n",
    "    mgl = np.array([140 ,50  ,0], np.uint8)\n",
    "\n",
    "    color_range = np.zeros((6, 3), dtype= np.uint8)\n",
    "\n",
    "    yh = np.array([40 ,255 ,255], np.uint8) #yh\n",
    "    yl = np.array([15 ,30 ,30], np.uint8) #yl\n",
    "\n",
    "    color_range[0, :] =  np.array([40, 255, 255], np.uint8) #yh\n",
    "    color_range[1, :] = np.array([15, 25, 25], np.uint8) #yl\n",
    "\n",
    "    color_range[2, :] = np.array([150,255,255], np.uint8) #bh\n",
    "    color_range[3, :] = np.array([100,100,100], np.uint8) #bl\n",
    "\n",
    "\n",
    "    color_range[4, :] = np.array([80 ,200 ,200], np.uint8) #gh 80 ,100 ,100\n",
    "    color_range[5, :] = np.array([45 ,50 ,0], np.uint8) # gl\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #plaatje205.jph\n",
    "    img = cv2.imread(file_str)\n",
    "    img1 = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = img.shape\n",
    "    #image = mpimg.imread(f,format='jpg')\n",
    "\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Threshold the HSV image to get only blue colors\n",
    "    #mask = cv2.inRange(hsv, yl, yh)\n",
    "    mask1 = cv2.inRange(hsv, mgl, mgh)\n",
    "\n",
    "    #mask3 = cv2.inRange(img1, wl, wh)\n",
    "    kernel = np.ones((4, 4), \"uint8\")\n",
    "    kernel1 = np.ones((8, 8), \"uint8\")\n",
    "    mask1 = cv2.erode(mask1, kernel)\n",
    "    mask1 = cv2.dilate(mask1, kernel1)\n",
    "\n",
    "    #mask = cv2.bitwise_or(mask1, mask2)\n",
    "    #target = cv2.bitwise_and(img1,img1, mask=mask)\n",
    "\n",
    "    _, contours, _ = cv2.findContours(mask1,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = max(contours, key = cv2.contourArea)\n",
    "\n",
    "    x, y, w, h = cv2.boundingRect(cnt)\n",
    "    \n",
    "\n",
    "    if (y - h)> 0 and ((y + 2*h) < height):\n",
    "        # first check \n",
    "        crop_img = hsv[y - h :y+(2*h), x:x+w]\n",
    "        lm = find_color(color_range, crop_img)\n",
    "\n",
    "    elif ((y - h)< 0) and ((y + 2*h) < height):\n",
    "        crop_img = hsv[y :y+(2*h), x:x+w]\n",
    "        #crop_img = hsv[y :y+(2*h), x:x+2*w]\n",
    "        lm = find_color(color_range, crop_img)\n",
    "    else: \n",
    "        crop_img = hsv[y - h:y, x:x+(1*w)]\n",
    "        lm = find_color(color_range, crop_img)\n",
    "\n",
    "    if not(lm == \"\"):\n",
    "        r, b = calculate_range_bearing(x, w, width)\n",
    "    return lm, r, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of example images are available for testing (Courtesy Tobias Garritsen):\n",
    "* [yp.jpg](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/yp.jpg)\n",
    "* [py.jpg](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/py.jpg)\n",
    "* [gp.jpg](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/gp.jpg)\n",
    "* [pg.jpg](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/pg.jpg)\n",
    "* [bp.jpg](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/bp.jpg)\n",
    "* [pb.jpg](https://staff.fnwi.uva.nl/a.visser/education/ProbabilisticRobotics/2018/pb.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect_landmark('./plaatje201.jpg')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-7.7*180/np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.tanh(94/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3:  FastSLAM\n",
    "\n",
    "Implement the FastSLAM 1.0 algorithm of Table 13.1 of the book. Analyze the performance and suggest possible\n",
    "improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to aware that theta is radius in log not degree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paticle:\n",
    "    def __init__(self, num_particles, num_landmark):\n",
    "        self.x = np.zeros((3, )) # x, y, theta\n",
    "        self.landmarks_mu = np.zeros((2* num_landmark)) # landmarks_x, landmarks_y\n",
    "        self.landmarks_Sigma = np.zeros((num_landmark, 2, 2))\n",
    "        #X_0= np.random.multivariate_normal([0, 0, 0], [[0.5, 0, 0],[0, 0.5, 0], [0, 0, 0.5]], 1)\n",
    "        self.x[0:3] = np.array([0, 0, 0])\n",
    "        self.weight = 1/float(num_particles)\n",
    "        self.seen_before = [False for t in range(num_landmark)]\n",
    "        \n",
    "    def __getitem__(self,key):\n",
    "        print(key)\n",
    "        return getattr(self,key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_line(s):        \n",
    "    s = s.split(' ')\n",
    "    if s[0] == 'mark':\n",
    "        return None, None\n",
    "\n",
    "    if s[0] == 'obs:':\n",
    "        x_obs = [float(s[2]) / 100,  # milimeters to decimeters\n",
    "                 float(s[3]) / 100,\n",
    "                 float(s[4]) * np.pi / 180] # degrees to radians\n",
    "\n",
    "        n_obs = int(s[5])\n",
    "        z_obs = np.zeros((2, n_landmarks))\n",
    "        for i in range(n_obs):\n",
    "            idx = 6 + i * 5\n",
    "            signature, x, y = s[idx+1:idx+4]                \n",
    "            signature = int(signature[0]), int(signature[2])\n",
    "            x = float(x) / 100 # milimeters to decimeters\n",
    "            y = float(y) * np.pi / 180 # degrees to radians\n",
    "\n",
    "            for landmark in range(n_landmarks):\n",
    "                if signature[0] == LID[0, landmark] and signature[1] == LID[1, landmark]:\n",
    "                    z_obs[:, landmark] = [x, y]\n",
    "\n",
    "        return x_obs, z_obs \n",
    "    print(s[0])\n",
    "    return None, None\n",
    "\n",
    "def distance(ax, bx, ay, by):\n",
    "    return np.sqrt((ax - bx)**2 + (ay - by)**2)\n",
    "\n",
    "def state_transition(x_, ut):\n",
    "    \n",
    "    # x_: 3x1 \n",
    "    v = ut[0]  # translation\n",
    "    da = ut[1] # delta angle\n",
    "    sa = ut[2] # start angle\n",
    "    \n",
    "    new_ut= np.random.multivariate_normal([v, da, sa], [[0.005, 0, 0],[0, 0.01, 0], [0, 0, 0.005]], 1)\n",
    "    \n",
    "    v = new_ut[:, 0]  # translation\n",
    "    da = new_ut[:, 1] # delta angle\n",
    "    sa = new_ut[:, 2] # start angle\n",
    "    \n",
    "    # predicted robot position mean\n",
    "    x_ = np.array(\n",
    "            [x_[0] + v*np.cos(x_[2]+sa),\n",
    "             x_[1] + v*np.sin(x_[2]+sa),\n",
    "             x_[2]+da]).reshape((3,))\n",
    "    \n",
    "    return x_\n",
    "\n",
    "def calculate_Jacobian(particle, landmark_id):\n",
    "    x = landmark_id * factor_dim\n",
    "    y = landmark_id * factor_dim + 1\n",
    "    \n",
    "    delta_x = particle.landmarks_mu[x] - particle.x[0]\n",
    "    delta_y = particle.landmarks_mu[y] - particle.x[1]\n",
    "    \n",
    "    expect_range =  np.sqrt((delta_x)**2 + (delta_y)**2)\n",
    "    expect_bearing = normalized_angle(np.arctan2(delta_y, delta_x) - particle.x[2])\n",
    "    \n",
    "    h = np.array([expect_range, expect_bearing ])\n",
    "    H = np.array(\n",
    "        [[delta_x/expect_range, delta_y/expect_range],\n",
    "         [-delta_y/(expect_range**2),  delta_x/(expect_range**2)]]).reshape(2, 2)\n",
    "    return h, H\n",
    "\n",
    "def normalized_angle(z_bearing):\n",
    "    if (z_bearing >np.pi) or (z_bearing<-np.pi):\n",
    "        while (z_bearing > np.pi):\n",
    "            z_bearing = z_bearing - (2*np.pi);\n",
    "\n",
    "        while (z_bearing < -np.pi):\n",
    "            z_bearing = z_bearing + (2*np.pi);\n",
    "    \n",
    "    return z_bearing\n",
    "    \n",
    "\n",
    "def measurement_update_SLAM(prev_particle, particle, zt):\n",
    "    H = np.zeros((factor_dim, dim, n_landmarks))    \n",
    "    foundx = np.zeros((dim, n_landmarks ))\n",
    "    K = np.zeros((dim, factor_dim, n_landmarks))\n",
    "    for landmark in range(n_landmarks):\n",
    "        \n",
    "        x_idx =landmark * factor_dim\n",
    "        y_idx =landmark * factor_dim + 1            \n",
    "        \n",
    "        particle.landmarks_mu[x_idx] = prev_particle.landmarks_mu[x_idx]\n",
    "        particle.landmarks_mu[y_idx] = prev_particle.landmarks_mu[y_idx]\n",
    "        particle.landmarks_Sigma[landmark, :] = prev_particle.landmarks_Sigma[landmark, :] \n",
    "        \n",
    "              \n",
    "        if zt[0, landmark] != 0:   # if Landmark is observed\n",
    "            \n",
    "            \n",
    "            \n",
    "            x_idx =landmark * factor_dim\n",
    "            y_idx =landmark * factor_dim + 1\n",
    "            if particle.landmarks_mu[x_idx] == 0:   # if landmark is never seen before:                \n",
    "                particle.landmarks_mu[x_idx] = particle.x[0] + (zt[0, landmark]*np.cos(zt[1, landmark] + particle.x[2]))\n",
    "                particle.landmarks_mu[y_idx] = particle.x[1] + (zt[0, landmark]*np.sin(zt[1, landmark] + particle.x[2]))   \n",
    "                \n",
    "                # initialize mean\n",
    "                h, H = calculate_Jacobian(particle, landmark)\n",
    "                inv_H = np.linalg.inv(H)\n",
    "                particle.landmarks_Sigma[landmark, :] = (inv_H.dot(Q)).dot(inv_H.T)\n",
    "                \n",
    "                # weight still stay the same as defult importance weight\n",
    "                \n",
    "            else:\n",
    "               \n",
    "                \n",
    "                ecptected_z, H = calculate_Jacobian(particle, landmark)\n",
    "                sig = particle.landmarks_Sigma[landmark, :]\n",
    "                Q_ = ((H.dot(sig)).dot(H.T)) + Q\n",
    "                inv_Q_ = np.linalg.inv(Q_)\n",
    "                # calculate the gain K\n",
    "                K = (sig.dot(H.T)).dot(inv_Q_)\n",
    "                z_diff = zt[:, landmark] - ecptected_z\n",
    "                z_diff[1] = normalized_angle(z_diff[1])\n",
    "                # update mean\n",
    "                particle.landmarks_mu[x_idx: y_idx + 1] = particle.landmarks_mu[x_idx: y_idx + 1] + K.dot(z_diff)\n",
    "                particle.landmarks_Sigma[landmark, :] = particle.landmarks_Sigma[landmark, :] - (K.dot(H)).dot(sig)\n",
    "                \n",
    "                # calculate the weigh\n",
    "                particle.weight = particle.weight* np.exp(-1/2* (z_diff.T.dot(np.linalg.inv(Q_)).dot(z_diff)))\\\n",
    "                                  /np.sqrt(np.linalg.det(2 * np.pi* Q_))\n",
    "                \n",
    "            \n",
    "    return particle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfilename = 'data/dlog.dat'; N = 51666;\n",
    "dim = 3\n",
    "factor_dim = 2\n",
    "#------------------------------------------------------------ data creation\n",
    "#\n",
    "# true robot position at t = 1\n",
    "xt = np.zeros((dim, N)) # x[:, i] = [x y angle]\n",
    "\n",
    "# user input at t = 1\n",
    "u = np.zeros((3, N))              # u = [trans, delta_angle, start_angle]\n",
    "\n",
    "# Landmark locations\n",
    "L2006 = np.array([[20,  20, -20, -20],\n",
    "                  [20, -20,  20, -20]])\n",
    "\n",
    "# You also need the following information about the landmark positions: \n",
    "# cyan:magenta -1500 -1000 magenta:cyan -1500 1000 magenta:green 0 -1000 green:magenta 0 1000 yellow:magenta 1500 -1000 magenta:yellow 1500 1000 \n",
    "# 0 -> green 1 -> magenta 2 -> yellow 3 -> blue\n",
    "L = np.array([[-15, -15,   0,  0,  15, 15],\n",
    "              [-10,  10, -10, 10, -10, 10]])\n",
    "LID = np.array([[3, 1, 1, 0, 2, 1], \n",
    "                [1, 3, 0, 1, 1, 2]])\n",
    "\n",
    "# expected robot location noise\n",
    "m_err = .1;\n",
    "Q = m_err*np.eye(2); \n",
    "\n",
    "z = np.zeros((2, N, L.shape[1]))\n",
    "n_landmarks = 6\n",
    "\n",
    "mark_t = []\n",
    "\n",
    "with open(logfilename, 'r') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "    for t, line in enumerate(lines):\n",
    "        x_obs, z_obs = parse_line(line)\n",
    "\n",
    "        if x_obs is None:\n",
    "            if t > 1 and t < N:\n",
    "                mark_t += [t]\n",
    "                xt[:,t] = xt[:,t-1] # skip the observation with a mark, use previous measurement\n",
    "                z[:,t] = z[:,t-1]\n",
    "            continue\n",
    "            \n",
    "        xt[:, t] = x_obs\n",
    "        z[:, t] = z_obs\n",
    "\n",
    "        if t > 0:\n",
    "            dx=xt[0,t]-xt[0,t-1]\n",
    "            dy=xt[1,t]-xt[1,t-1]\n",
    "\n",
    "            u[2,t] = np.arctan2(dy,dx) - xt[2,t-1] # start_angle\n",
    "            u[2,t] = np.mod(u[2,t] + np.pi, 2*np.pi) - np.pi\n",
    "            u[1,t] = xt[2,t]-xt[2,t-1]; # diff_angle\n",
    "            u[1,t] = np.mod(u[1,t] + np.pi, 2*np.pi) - np.pi # FIX angle difference range\n",
    "            u[0,t] = np.sqrt(dx*dx+dy*dy) # translation\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------- a prioris\n",
    "x_ = xt[:, 1] # a priori x = true robot position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples_from_newset(particles, sampling_number):\n",
    "    elements = np.arange(sampling_number);\n",
    "    #weights = X[:, 2]/X[:, 2].sum();\n",
    "    weights = np.zeros((sampling_number, ))\n",
    "    for idx in range(sampling_number):\n",
    "        weights[idx] = (particles[idx].weight)\n",
    "     \n",
    "    weights = weights/weights.sum()\n",
    "    for idx in range(sampling_number):\n",
    "        particles[idx].weight = weights[idx]\n",
    "        \n",
    "    idx_chosen = (choice(elements, p=weights, size=sampling_number));\n",
    "    \n",
    "    new_particles = []\n",
    "    for idx, idx_val in enumerate(idx_chosen):\n",
    "        particles[idx_val].weight = 1.0/M\n",
    "        new_particles += [particles[idx_val]]\n",
    "    return new_particles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100; #sampling number\n",
    "T = 5000  # total timestamp\n",
    "Features = 6\n",
    "\n",
    "mark_ts = mark_t[-6:-1]\n",
    "mark_p = np.zeros((5, 3))\n",
    "mark_ts_idx = 0\n",
    "\n",
    "particles = [[Paticle(num_particles=M, num_landmark=Features) for x in range(M)] for y in range(2)]\n",
    "for t in range(N-1):\n",
    "    #print('posterior at time = ', t+1, ' before updating with measurement');\n",
    "    \n",
    "    for m in range(M):  # loop over all particles\n",
    "        # calculate transition probaility:\n",
    "        # update state\n",
    "        \n",
    "        particles[1][m].x = state_transition(particles[0][m].x, u[:, t])\n",
    "        particles[1][m].x[2] = normalized_angle(particles[1][m].x[2])\n",
    "        particles[1][m] = measurement_update_SLAM(particles[0][m], particles[1][m], z[:, t, :])\n",
    "        \n",
    "    # initilize new particle set    \n",
    "    particles[0] = draw_samples_from_newset(particles[1], M)\n",
    "    particles[1] = [Paticle(num_particles=M, num_landmark=Features) for x in range(M)]\n",
    "    \n",
    "    if mark_ts[mark_ts_idx] == t:\n",
    "        mark_p[mark_ts_idx % 5, :] = particles[0][0].x\n",
    "        print(mark_p)\n",
    "        mark_ts_idx = mark_ts_idx + 1\n",
    "    \n",
    "    if (t % 1000) == 0:\n",
    "        print(t)\n",
    "    if ((mark_ts_idx% 5) == 0 and not (mark_ts_idx == 0)) or ((t % 5000) == 0):\n",
    "        fig, ax = plt.subplots()\n",
    "        for idx in range(80):\n",
    "            ax.plot(particles[0][idx].landmarks_mu[0:12:2],particles[0][idx].landmarks_mu[1:12:2], 'bo')#, label=\"Prediction\");   \n",
    "            if mark_ts_idx == 5:\n",
    "                ax.plot(mark_p[:, 0],mark_p[:, 1], 'ro')#, label=\"Prediction\");       \n",
    "        ax.set_xlim(-20, 20)\n",
    "        ax.set_ylim(-20, 20)\n",
    "        plt.xlabel(\"x\")\n",
    "        plt.ylabel(\"y\")\n",
    "        plt.title(\"Visualization of uncertainty of localization of EKL for SLAM\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        if mark_ts_idx == 5:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hand-In\n",
    "\n",
    "You can work on this assignment up to four people. When you have completed the\n",
    "assignment, each should upload your solution to Canvas (with clear pointers to your team-mates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
